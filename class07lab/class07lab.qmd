---
title: "Class 7:Machine Learning 1"
author: "Yerin Go (A16272901)"
format: pdf
editor: visual
---

#Clustering Methods

The broad goal here is to find groupings (clusters) in your input data

##kmeans 
 
First, let's make up some data to cluster. 

```{r}
x <- rnorm(1000)
hist(x)
```
Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3 

```{r}
tmp <- c(rnorm(30, mean =-3), rnorm(30, mean = 3))
tmp
```


I will not make a wee x and y dataset with 2 groups of points. 

```{r}
rev(c(1:5))
```


```{r}
x <- cbind(x=tmp, y=rev(tmp))
x
```
```{r}
plot(x)
```
```{r}
k <-kmeans(x, centers=2)
k
```
>. Q. From your result object 'k' how many points are in each cluster? 

```{r}
k$size
```

>. Q. What "component" of your results object details the cluster membership? 

```{r}
k$cluster
```

>. Q. Cluster centers? 

```{r}
k$centers 
```

>. Q. Plot of our clustering results? 

```{r}
plot(x, col=k$cluster)
points(k$centers, col="blue", pch= 15, cex=2)
```
We can cluster into 4 groups. 
```{r}
# kmeans 
k4 <- kmeans(x, centers=4)
#plot results 
plot(x, col=k4$cluster)
```
A big limitation of kmeans is that it does waht you ask even if you ask for silly clusters. 
# Hierarchical Clustering 

The main base R function for Hierarchical Clustering is `hclust()` 
Unlike `kmeans()` you cannot just pass it your data as input. You first need to calculate a distance matrix. 
```{r}
d <- dist(x)
hc <-hclust(d)
hc
```
Use `plot()` to view results. 
```{r}
plot(hc)
abline(h=10, col="red")
```
To make the "cut" and get our cluster membership vector we can use the `cutree()` function. 
```{r}
grps <- cutree(hc, h=10)
grps
```
Make a plot of our data colored by hclust results. 
```{r}
plot(x, col=grps) 
```
#Principle Component Analysis (PCA)
Here we will do PCA on some food data from the UK. 
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
plot(x)
```
#Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(x)
```


```{r}
nrow(x)
```
```{r}
ncol(x)
```
##PCA to the rescue 
The main "base" R function for PCA is called `prcomp()` 
Here we need to take the transpose of our input as we want the countries in the rows and foods as the columns. 
```{r}
pca <- prcomp(t(x))
summary(pca)
```
>. Q. How much variance is captured to 2 PCs 
96.5% 
 
To make our main "PC score plot" or "PC1 vs. PC2 plot" or "PC plot" or "Ordination Plot".  
```{r}
attributes(pca)
```
We are after the `pca$x` result component to make out main PCA plot 
```{r}
pca$x
```
```{r}
mycols <- c("orange", "red", "blue","darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab= "PC1 (67.4%)", ylab= "PC2 (29%)")
```
Another important result from PCA is how the original variables (in this case the foods) contribute to the PCs. 
This is contained in the `pca$rotation` object- folks often call this the "loadings" or "contributions" to the PCs. 

```{r}
pca$rotation[,1]
```
We can make a plot along PC1. 
```{r}
library(ggplot2)
contrib <- as.data.frame(pca$rotation)
ggplot(contrib) + 
 aes(PC1, rownames(contrib))+ 
  geom_col()

```

#Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer adding the part of the code `row.names=1` to the of the line of code 
`x <- read.csv(url, row.names=1)`. This approach is more robust than using the 
`rownames(x) <- x[,1]x <- x[,-1] head(x)` because everything I run the code the second way, it will override the function, therefore deleting the relative first row each time the code is run. 
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
#Q3: Changing what optional argument in the above barplot() function results in the following plot? 
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```